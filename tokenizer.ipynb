{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import sentencepiece as spm\n",
    "from datasets import load_dataset\n",
    "\n",
    "def train_sentencepiece_from_huggingface():\n",
    "    # 1. Load Dataset in Streaming Mode\n",
    "    # 'streaming=True' is crucial. It downloads data on the fly, \n",
    "    # so you don't need 50GB of RAM or disk space.\n",
    "    print(\"Setting up stream...\")\n",
    "    dataset = load_dataset(\"ncduy/mt-en-vi\", split=\"train\", streaming=True)\n",
    "    \n",
    "    # 2. Define a Python Generator\n",
    "    # SentencePiece needs an iterator that yields strings.\n",
    "    # We loop through the dataset and yield the 'text' column.\n",
    "    def batch_iterator(dataset_stream, limit=2800000):\n",
    "        count = 0\n",
    "        for i, item in enumerate(dataset_stream):\n",
    "            if count >= limit:\n",
    "                break\n",
    "            \n",
    "            # Extract the text content\n",
    "            text = item.get(\"en\", \"\")\n",
    "            \n",
    "            # Basic cleaning (optional but recommended)\n",
    "            # Remove newlines to prevent sentence splitting issues if needed\n",
    "            text = text.replace(\"\\n\", \" \") \n",
    "            \n",
    "            if text.strip():\n",
    "                yield text\n",
    "                count += 1\n",
    "                \n",
    "        print(f\"Processed {count} sentences for training.\")\n",
    "\n",
    "    # 3. Train SentencePiece using 'sentence_iterator'\n",
    "    # Note: We use 'sentence_iterator' instead of 'input'\n",
    "    print(\"Starting training (this might take a while)...\")\n",
    "    \n",
    "    # Create the iterator\n",
    "    data_iter = batch_iterator(dataset, limit=2000000) # Adjust limit as needed\n",
    "    \n",
    "    spm.SentencePieceTrainer.train(\n",
    "        sentence_iterator=data_iter,\n",
    "        model_prefix='english_toknizer_spm', # Output filename (.model)\n",
    "        vocab_size=32000,                    # As discussed for IWSLT\n",
    "        character_coverage=0.9995,          # Good for Vietnamese\n",
    "        model_type='unigram',               # Best for Translation\n",
    "        pad_id=3,                           # Crucial for PyTorch\n",
    "        unk_id=0,\n",
    "        bos_id=1,\n",
    "        eos_id=2,\n",
    "        input_sentence_size=2000000,        # Buffer size for shuffling\n",
    "        shuffle_input_sentence=True         # Randomize samples for better training\n",
    "    )\n",
    "    \n",
    "    print(\"Training finished! Saved 'vietnamese_wiki_spm.model'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_sentencepiece_from_huggingface()"
   ],
   "id": "8d96860ea7230f9a"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
